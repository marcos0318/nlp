{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of word2vec\n",
    "Distributed Representations of Words and Phrases and their Compositionality. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "\"\"\"read the downloaded data and build the dataset\"\"\"\n",
    "with open(\"text8\") as f:\n",
    "    data = tf.compat.as_str(f.readline()).split()\n",
    "\n",
    "print('Data size', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to do it without using tensorflow, just numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    @x: numpy array\n",
    "    if x is a vector, return the softmax vector\n",
    "    else if is a matrix, return the softmax vectors of the rows, as the row vectors of a that matrix\n",
    "    \"\"\"\n",
    "    if len(x.shape) > 1:\n",
    "        # matrix\n",
    "        # have to use the transpose, in case the row and col have the same size\n",
    "        upper = np.exp((x.transpose() - np.max(x, axis=1)).transpose())\n",
    "        down = np.sum(upper, axis=1)\n",
    "        result = (upper.transpose() / down).transpose()\n",
    "    else:\n",
    "        # vector\n",
    "        result = np.exp(x-np.max(x)) \n",
    "        result = result / np.sum(result)\n",
    "    return result\n",
    "\n",
    "# sigmoid function\n",
    "sigmoid = lambda x: 1.0/(1+np.exp(-x))\n",
    "# s is the sigmoid value of x\n",
    "sigmoid_grad = lambda s : s * (1 - s)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"@tests\"\"\"\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    test1 = softmax(np.array([1, 2]))\n",
    "    print (test1)\n",
    "    ans1 = np.array([0.26894142, 0.73105858])\n",
    "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test2 = softmax(np.array([[1001, 1002], [3, 4]]))\n",
    "    print (test2)\n",
    "    ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test3 = softmax(np.array([[-1001, -1002]]))\n",
    "    print (test3)\n",
    "    ans3 = np.array([0.73105858, 0.26894142])\n",
    "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    print (\"You should be able to verify these results by hand!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
